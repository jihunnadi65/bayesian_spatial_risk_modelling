str_c("충청북도 ", municipality),
# assign "충청남도" to its municipalities
str_detect(municipality, "^(천안시|공주시|보령시|아산시|서산시|논산시|계룡시|당진시|금산군|부여군|서천군|청양군|홍성군|예산군|태안군)$") ~
str_c("충청남도 ", municipality),
# assign "전라북도" to its municipalities
str_detect(municipality, "^(전주시|군산시|익산시|정읍시|남원시|김제시|완주군|진안군|무주군|장수군|임실군|순창군|고창군|부안군)$") ~
str_c("전라북도 ", municipality),
# assign "전라남도" to its municipalities
str_detect(municipality, "^(목포시|여수시|순천시|나주시|광양시|담양군|곡성군|구례군|고흥군|보성군|화순군|장흥군|강진군|해남군|영암군|무안군|함평군|영광군|장성군|완도군|진도군|신안군)$") ~
str_c("전라남도 ", municipality),
# assign "경상북도" to its municipalities
str_detect(municipality, "^(포항시|경주시|김천시|안동시|구미시|영주시|영천시|상주시|문경시|경산시|의성군|청송군|영양군|영덕군|청도군|고령군|성주군|칠곡군|예천군|봉화군|울진군|울릉군)$") ~
str_c("경상북도 ", municipality),
# assign "경상남도" to its municipalities
str_detect(municipality, "^(진주시|통영시|사천시|김해시|밀양시|거제시|양산시|창원시|의령군|함안군|창녕군|고성군|남해군|하동군|산청군|함양군|거창군|합천군)$") ~
str_c("경상남도 ", municipality),
# assign "제주도" to its municipalities
str_detect(municipality, "^(제주시|서귀포시)$") ~
str_c("제주도 ", municipality),
TRUE ~ municipality # there shouldn't be any exceptions
)) %>%
# manually handle name discrepancies not covered above & errors introduced by the above due to duplicate municipality names in different provinces
dplyr::mutate(municipality =
ifelse(str_detect(municipality, "대구광역시 군위군"),
"경상북도 군위군", municipality)) %>%
dplyr::mutate(municipality =
ifelse(row_number() == 220 & str_detect(municipality, "강원도 고성군"), "경상남도 고성군", municipality)) %>%
dplyr::mutate(municipality =
ifelse(row_number() == 100 & str_detect(municipality, "광주광역시시"), "경기도 광주시", municipality)) %>%
dplyr::rename(municipality_clean = municipality) %>%
dplyr::mutate(unemployment_rate = as.numeric(unemployment_rate))
# handle missing row for Sejong Special Self-Governing City
# since Sejong's unemployment rate is missing, we impute it using the the average unemployment rate of neighbouring provinces (충청북도, 충청남도, 대전)
chungcheong_avg_unemployment <- unemployment_rate_clean %>%
filter(str_detect(municipality_clean, "충청북도|충청남도|대전광역시")) %>%
summarise(mean_unemployment = mean(unemployment_rate, na.rm = TRUE)) %>%
pull(mean_unemployment)
# if Sejong is missing, impute it with the calculated average
if(!any(unemployment_rate_clean$municipality_clean == "세종특별자치시")) {
sejong_unemployment <- tibble(
municipality_clean = "세종특별자치시",
unemployment_rate = round(chungcheong_avg_unemployment, 1)
)
# add Sejong to the dataset
unemployment_rate_clean <- unemployment_rate_clean %>%
bind_rows(sejong_unemployment)
}
# load unmet medical needs data
# reads the data from excel file containing unmet medical needs by municipality in 2022
unmet_medical_needs <- read_excel(here::here("variables", "predictor variable", "unmet_medical_needs_rate_by_municipality_2022_2024.xlsx"), col_names=TRUE) %>%
# select only relevant columns
dplyr::select("시군구별(1)", "시군구별(2)", "2022") %>%
# translate columns to English
dplyr::rename(
municipality_level_1 = "시군구별(1)", # higher administrative level (province / special/ metropolitan city)
municipality_level_2 = "시군구별(2)", # lower administrative level (city / county / district)
unmet_medical_needs_rate = "2022"
)
# clean and standardise municipality names
unmet_medical_needs_clean <- unmet_medical_needs %>%
# remove rows where 'municipality_level_2' is '소계' (subtotal)
dplyr::filter(municipality_level_2 != "소계") %>%
# standardise province names to be consisent with their names in 2022
dplyr::mutate(municipality_level_1 = str_replace(municipality_level_1, "^전북특별자치도", "전라북도"),
municipality_level_1 = str_replace(municipality_level_1, "^강원특별자치도", "강원도"),
municipality_level_1 = str_replace(municipality_level_1, "^제주특별자치도", "제주도")
) %>%
# combine municipality_level_1 with municipality_level_2 for to get full municipality names
dplyr::mutate(municipality_clean = paste(municipality_level_1, municipality_level_2)) %>%
# manually handle the error introduced by above for the special case of Sejong Special Self-Governing City
dplyr::mutate(municipality_clean =
ifelse(str_detect(municipality_clean, "세종특별자치시 세종시"),
"세종특별자치시", municipality_clean)) %>%
# retain only relevant columns
dplyr::select(municipality_clean, unmet_medical_needs_rate) %>%
# conveert unmet medical needs rate colun to numeric type
dplyr::mutate(unmet_medical_needs_rate = as.numeric(unmet_medical_needs_rate))
# load municipality population data
# reads resident registration population data in 2022 by municipality from excel file
municipality_population_2022 <- read_excel(here::here("variables", "predictor variable", "resident_registration_population_by_administrative_district_2022.xlsx"), col_names=TRUE) %>%
# select only relevant columns
dplyr::select("행정기관", "총인구수", "남자 인구수", "여자 인구수") %>%
# translate columns to English
dplyr::rename(
municipality = "행정기관",
total_population = "총인구수",
male_population = "남자 인구수",
female_population = "여자 인구수"
) %>%
# standardise municipality names
dplyr::mutate(municipality = str_replace(municipality, "^제주특별자치도", "제주도"),
# convert population values from strings with commas to numeric
total_population = as.numeric(str_replace_all(total_population, ",", "")),
male_population = as.numeric(str_replace_all(male_population, ",", "")),
female_population = as.numeric(str_replace_all(female_population, ",", "")),) %>%
# filter only for rows where 1. the municipality is Sejong or 2. the municipality name contains exactly one space, ensuring correct administrative level
dplyr::filter(municipality == "세종특별자치시" | str_count(municipality, "\\s") == 1) %>%
# remove duplicate entries for Sejong
dplyr::distinct() %>%
# rename municipality column to municipality_clean for consistency across datasets
dplyr::rename(municipality_clean = municipality)
# read in a mapping file that converts Korean municipality names to English
kr_to_en_mapping <- read_excel(here::here("mapping", "korean_to_english_mapping.xlsx"))
# merge all predictor variables with the suicide rate data
model_df <- suicide_rate_municipality %>%
# join single-person household ratio
dplyr::left_join(., single_person_household_clean, by=c("name" = "municipality_clean")) %>%
# join stress awareness rate
dplyr::left_join(., stress_awareness_rate_clean, by=c("name" = "municipality_clean")) %>%
# join unemployment rate
dplyr::left_join(., unemployment_rate_clean, by=c("name" = "municipality_clean")) %>%
# join unmet medical needs rate
dplyr::left_join(., unmet_medical_needs_clean, by=c("name" = "municipality_clean")) %>%
# join municipality population data (for expected values & suicide counts)
dplyr::left_join(., municipality_population_2022, by=c("name" = "municipality_clean")) %>%
# join Korean-to-English name mapping
dplyr::left_join(., kr_to_en_mapping, by=c("name" = "municipality_kr")) %>%
# compute additional features
# calculate municipality area (in km²)
dplyr::mutate(., area = as.numeric(set_units(st_area(geometry), "km^2"))) %>%
# calculate population density (people per km²)
dplyr::mutate(population_density_km2 = round((total_population / area),1)) %>%
# calculate estimated suicide count
# we use the suicide rate (per 100,000 people) to estimate actual suicide counts from 2022 population
# round to the nearest integer for modelling purposes
dplyr::mutate(suicide_count = as.integer(round(total_population * suicide_rate / 100000, 0))) %>%
# select rename relevant columns and keep only necessary variables for modelling
dplyr::select(code, municipality_en, suicide_count, suicide_rate, single_person_household_ratio, stress_awareness_rate, unemployment_rate, unmet_medical_needs_rate, population_density_km2, total_population) %>%
# rename 'municipality_en' to 'name'
dplyr::rename(name = municipality_en)
# # quick diagnosis with variance-to-mean ratio
var(model_df$suicide_count) / mean(model_df$suicide_count)
var(model_df$suicide_count)
# # Create histogram for suicide counts
ggplot(model_df, aes(x = suicide_count)) +
geom_histogram(binwidth = 5, fill = "darkblue", color = "white", alpha = 0.7) +
geom_density(aes(y = ..count.. * 5), color = "red", size = 1) +
labs(title = "Reported Number of Suicide in Korean Municipalities",
x = "Suicide Counts",
y = "Frequency") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10))
# # see lowest count
min(model_df$suicide_count)
# see highest count
max(model_df$suicide_count)
# since we are considering using the Negative Binomial Poisson Regression, let us estimate the over-dispersion parameter using the glm.nb() function
# # fit negative binomial regression null model
nb_model <- glm.nb(suicide_count ~ 1, data=model_df)
# # extract theta
theta <- nb_model$theta
theta
# the estimated over-dispersion parameter is 1.28132
# the theta (dispersion) parameter in a negatival binomial model controls the degree of overdispersion relative to a Poisson distribution
# a higher theta value suggests less overdisperison, while a lower theta value suggests more overdispersion
# my value is 1.28132, meaning the suicide_count data is overdispersed but not extremely so
# in a Poisson regression, the variance equals the mean (Var(Y) = E(Y))
# in a negative binomial model, variance is greater than the mean: Var(Y) = mu + mu^2 / theta
# NB model can be used to characterise count data where the majority of data points are clustered toward lower values of a variable
# When choosing between a negative binomial model and a Poisson model, use a negative binomial model if your count data exhibits overdispersion (variance greater than the mean), while a Poisson model is suitable when the variance is roughly equal to the mean in your count data; essentially, the negative binomial model is a more flexible option that can handle situations where the Poisson model's assumptions are not met
# Three types of Poisson models
#
# - Standard Poisson Regression
# - Negative Binomial Poisson Regression
# - Zero-Inflated Poisson Regression
#
# The implementation of these models are highly dependent on how the frequency distribution of the count response variable are displayed
#
# If it resembles a normal curve - then use the standard version
# Otherwise, use the Negative Binomial Poisson regression if there is any evidence of over-dispersion
# When there is an inflation of zero counts in the dataset, you will have to use the Zero-Inflated Poisson model
# enable parallel processing in RStudio for Stan
# parallel::detectCores() automatically detects the number of CPU cores available on my local machine
options(mc.cores = parallel::detectCores())
# rtan_options(auto_write = TRUE) saves compiled models to avoid redundant compilation
rstan_options(auto_write = TRUE)
# in order to estimate the risk of casualties due to suicide across Korean municipalities, we will need to first obtain a column that contains estimated expected number of casualties. This is derived from the total_population column.
# n.strata = 1 means that no stratification is being applied - the entire dataset is treated as a single stratum
# this will be used as an offset in our spatial model
model_df$expected_num <- round(expected(population = model_df$total_population, cases = model_df$suicide_count, n.strata = 1), 0)
# convert model_df to a spatial object
# poly2nb() from the spdep package works with sp objects, not sf objects
model_df_sp <- as(model_df, "Spatial")
# create standard adjacency matrix using Queen contiguity
# Queen's contiguity considers any shared boundary or shared vertex as a neighbour
# the 'snap = 200' argument ensures that near-touching polygons within 200m are considered contiguous
adj_list <- poly2nb(model_df_sp, queen = TRUE, snap = 200)
# inspect adj_list
# the list has 6 subgraphs and 4 isoalted regions (59, 177, 184, 190) with no links
adj_list
# we need to convert the list-based adjacency strucuture into an igraph object to analyse connected components
graph_adj <- graph_from_adj_list(adj_list, mode = "all")
# identify subgraph components
subgraph_labels <- components(graph_adj)$membership
model_df$subgraph <- as.factor(subgraph_labels)
# find the largest subgraph (mainland Korea)
largest_subgraph <- which.max(table(subgraph_labels))
# identify all smaller disconnected subgraphs
# this returns a list of subgraph IDs
disconnected_subgraphs <- unique(subgraph_labels[subgraph_labels != largest_subgraph])
# inspect disconnected subgraphs
disconnected_subgraphs  # Should return a list of subgraph IDs
# inspect the current state of subgraph connectivity
ggplot() +
geom_sf(data = model_df, aes(fill = as.factor(subgraph_labels)), color = "black") +
scale_fill_viridis_d(option = "turbo", name = "Subgraph")  +
ggtitle("Current Subgraph Connectivity in South Korea") +
theme_minimal()
# convert centroids to an sf object
centroids <- st_centroid(model_df$geometry)
centroids_sf <- st_as_sf(data.frame(id = 1:length(centroids), geometry = centroids))
# iterate through each disconnected subgraph
for (subgraph in disconnected_subgraphs) {
# find polygons in the current subgraph
subgraph_polygons <- which(subgraph_labels == subgraph)
# find polygons in the largest subgraph (mainland)
mainland_polygons <- which(subgraph_labels == largest_subgraph)
# find nearest polygon in the mainland for each polygon in the subgraph
closest_mainland <- st_nearest_feature(centroids_sf[subgraph_polygons, ], centroids_sf[mainland_polygons, ])
# convert closest_mainland to integer indices
closest_mainland <- as.integer(closest_mainland)
# update adjacency list to connect subgraph polygons to mainland
for (i in seq_along(subgraph_polygons)) {
island <- subgraph_polygons[i]
mainland <- closest_mainland[i]
# ensure bidirectional connection
adj_list[[island]] <- unique(c(adj_list[[island]], mainland))
adj_list[[mainland]] <- unique(c(adj_list[[mainland]], island))
}
}
# identify which nodes have invalid `0` entries (0, 58) for example
invalid_nodes <- which(sapply(adj_list, function(x) any(x == 0)))
# print the invalid nodes
print(invalid_nodes)  # Should show 59, 177, 184, 190, etc.
# loop through adj_list and remove zeroes
adj_list <- lapply(adj_list, function(x) x[x != 0])
# verify that 0s are gone
nodes_to_check <- c(59, 177, 184, 190)
print(adj_list[nodes_to_check])
# recompute the graph after linking all subgraphs
graph_adj <- graph_from_adj_list(adj_list, mode = "all")
subgraph_labels <- components(graph_adj)$membership
model_df$subgraph <- as.factor(subgraph_labels)
# check if any disconnected subgraphs remain
remaining_disconnected <- unique(subgraph_labels[subgraph_labels != largest_subgraph])
if (length(remaining_disconnected) == 0) {
message("All regions are now fully connected.")
} else {
message("Some subgraphs are still disconnected: ", paste(remaining_disconnected, collapse = ", "))
}
# visualise the updated subgraph connectivity
ggplot() +
geom_sf(data = model_df, aes(fill = as.factor(subgraph_labels)), color = "black") +
scale_fill_viridis_d(option = "turbo", name = "Subgraph") +
ggtitle("Updated Subgraph Connectivity in South Korea") +
theme_minimal()
# convert the updated graph to an adjacency matrix
adjacency_matrix <- as_adjacency_matrix(graph_adj, sparse = TRUE)
adjacency_matrix <- Matrix::Matrix(as(adjacency_matrix, "sparseMatrix") > 0, sparse = TRUE)
# inspect
adjacency_matrix
# save the fitted model locally
# saveRDS(icar_poisson_fit, file = "icar_poisson_fit.rds")
# load the model later:
icar_poisson_fit <- readRDS("icar_poisson_fit.rds")
# we can see our estimated results for alpha, beta, and sigma
# all rHAT values are below 1.05, indicating that the iteration performed well
options(scipen = 999)
summary(icar_poisson_fit, pars=c("alpha", "beta", "sigma"), probs=c(0.025, 0.975))$summary
# print full table to avoid some rows from being omitted.
options(max.print = 100000)
# print the results
print(icar_poisson_fit, pars=c("alpha", "beta", "rr_alpha", "rr_beta", "rr_mu", "sigma"), probs=c(0.025, 0.975))
# rHAT- all values are ≈ 1, which means the model has converged well across all chains
# effective samples size (n_eff) are all quite lare, indicating low autocorrelation in the Markov Chains (larger n_eff means more precise estimates)
# single_person_household_ratio and unmet_medical_needs_rate have slightly positive associations with suicide rate
# unemployment_rate has a borderline significant, negative association with suicide rate
# stress_awareness_rate has a small, borderline significant negative effect close to 0
# relative risk estimates
# single_person_household_ratio (1.01) -> 1% increase in risk per unit increase
# stress_awareness_rate (0.99) -> 1% decrease in risk per unit increase
# unemployment_rate (0.96) -> 4% decrease in risk per unit increase (strongest effect)
# unmet_medical_needs_rate (1.01) -> 1% increase in risk per unit increase
# alpha (global baseline)
# alpha of -0.20 (95% CrI: -0.45 - 0.07) indicates a small negative baseline risk for suicide rates
# sigma (standard deviation)
# sigma of 0.21 (95% CrI: 0.16 to 0.27) indicates the overall variance in the model
# rapid diagnostics of the rHATs
# diagnostic check on the rHats by putting everything into a data frame
diagnostic_checks <- as.data.frame(summary(icar_poisson_fit, pars=c("alpha", "beta", "rr_alpha", "rr_beta", "rr_mu", "sigma", "phi", "lp__"), probs=c(0.025, 0.5, 0.975))$summary)
# create binary variable
diagnostic_checks$valid <- ifelse(diagnostic_checks$Rhat < 1.05, 1, 0)
# tabulate it
table(diagnostic_checks$valid)
# all output parameters have an rHAT < 1.05
# it is always good to run about 10000, 15000 or more iterations, as usually shorter iterations yield low effective sample sizes after warn-up samples are discarded - this my lead to complications that cause rHATs to be above 1.05
# extraction of area-specific relative risks
head(summary(icar_poisson_fit, pars = c("rr_mu"), probs = c(0.025, 0.975))$summary)
# we see the relative risk (RR) estimates for the first areas under the column mu with their corresponding credibility limits (2.5% and 97.5%)
# extract key posterior results for the generated quantities
relative_risk_results <- as.data.frame(summary(icar_poisson_fit, pars=("rr_mu"), probs = c(0.025, 0.975))$summary)
# clean this table up
row.names(relative_risk_results) <- 1:nrow(relative_risk_results)
# check for validity of rHATs
relative_risk_results$valid <- ifelse(relative_risk_results$Rhat < 1.05, 1, 0)
# rearrange the column into order
relative_risk_results <- relative_risk_results[, c(1, 4, 5, 7, 8)]
# rename the columns appropriately
colnames(relative_risk_results)[1] <- "rr"
colnames(relative_risk_results)[2] <- "rrlower"
colnames(relative_risk_results)[3] <- "rrupper"
colnames(relative_risk_results)[4] <- "rHAT"
# inspect clean table
head(relative_risk_results)
# insert these columns into our model_df
model_df$rr <- relative_risk_results[, "rr"]
model_df$rrlower <- relative_risk_results[, "rrlower"]
model_df$rrupper <- relative_risk_results[, "rrupper"]
# these relative will allow us to see the mapped risks of suicide across South Korean municipalities
# as well also want a supporting map indicating whether the risks are significant or not, we create an extra column in model_df called significance
model_df$significance <- NA
model_df$significance[model_df$rrlower < 1 & model_df$rrupper > 1] <- 0 # not significant
model_df$significance[model_df$rrlower == 1 | model_df$rrupper == 1] <- 0 # not significant
model_df$significance[model_df$rrlower > 1 & model_df$rrupper > 1] <- 1 # significant increase
model_df$significance[model_df$rrlower < 1 & model_df$rrupper < 1] <- -1 # significant decrease
# map design for relative risk - you want to understand or get a handle on what the distribution for risks look like
summary(model_df$rr)
hist(model_df$rr)
# Refined risk categories for better separation
risk_category_list <- c(
"≤0.75",
"0.76 to 0.85",
"0.86 to 0.95",
"0.96 to 1.00",
"1.01 to 1.05",
"1.06 to 1.10",
"1.11 to 1.18",
"1.19 to 1.25",
"1.26 to 1.50",
">1.50"
)
# Define updated colour palette
RRPalette <- c(
"#4575B4",  # Dark blue (≤ 0.75)
"#5EA9D4",  # Medium blue (0.76 - 0.85)
"#74ADD1",  # Light blue (0.86 - 0.95)
"#ABD9E9",  # Pale blue (0.96 - 1.00)
"#E0F3F8",  # Very light blue (1.01 - 1.05)
"#FFFFBF",  # White-yellow (1.06 - 1.10)
"#FEE08B",  # Light orange (1.11 - 1.18)
"#FDAE61",  # Orange (1.19 - 1.25)
"#F46D43",  # Dark orange-red (1.26 - 1.50)
"#A50026"   # Deep red (> 1.50)
)
# Assign refined risk categories
model_df$relative_risk_cat <- NA
model_df$relative_risk_cat[model_df$rr <= 0.75] <- -4
model_df$relative_risk_cat[model_df$rr > 0.75 & model_df$rr <= 0.85] <- -3
model_df$relative_risk_cat[model_df$rr > 0.85 & model_df$rr <= 0.95] <- -2
model_df$relative_risk_cat[model_df$rr > 0.95 & model_df$rr <= 1.00] <- -1
model_df$relative_risk_cat[model_df$rr > 1.00 & model_df$rr <= 1.05] <- 0
model_df$relative_risk_cat[model_df$rr > 1.05 & model_df$rr <= 1.10] <- 1
model_df$relative_risk_cat[model_df$rr > 1.10 & model_df$rr <= 1.18] <- 2
model_df$relative_risk_cat[model_df$rr > 1.18 & model_df$rr <= 1.25] <- 3
model_df$relative_risk_cat[model_df$rr > 1.25 & model_df$rr <= 1.50] <- 4
model_df$relative_risk_cat[model_df$rr > 1.50] <- 5
# check the category distribution
table(model_df$relative_risk_cat)
# map of relative risk
rr_map <- tm_shape(model_df) +
tm_fill("relative_risk_cat",
style = "cat",
title = "Relavtive Risk",
palette = RRPalette,
labels = risk_category_list) +
tm_shape(model_df) +
tm_polygons(alpha = 0.05) +
# tm_text("name", size = "AREA") +
tm_layout(frame = FALSE,
legend.outside = TRUE,
legend.title.size = 0.8,
legend.text.size = 0.7) +
tm_compass(position = c("right", "top")) +
tm_scale_bar(position = c("right", "bottom")) +
tm_layout(
main.title = "Relative Risk of Suicide Across South Korean Municipalities",
main.title.size = 1.2,
main.title.position = c("center", "top"),
frame = FALSE,
legend.outside = TRUE,
legend.title.size = 0.8,
legend.text.size = 0.7
)
# inspect
rr_map
# map of significance regions
sg_map <- tm_shape(model_df) +
tm_fill("significance",
style = "cat",
title = "Significance Categories",
palette = c("#33a6fe", "white", "#fe0000"),
labels = c("Significantly low", "Not Significant", "Significantly high")) +
tm_shape(model_df) +
tm_polygons(alpha = 0.05) +
# tm_text("name", size = "AREA") +
tm_layout(frame = FALSE,
legend.outside = TRUE,
legend.title.size = 0.8,
legend.text.size = 0.7) +
tm_compass(position = c("right", "top")) +
tm_scale_bar(position = c("right", "bottom")) +
tm_layout(
main.title = "Significance of Suicide Risk Across South Korean Municipalities",
main.title.size = 1.2,
main.title.position = c("center", "top"),
frame = FALSE,
legend.outside = TRUE,
legend.title.size = 0.8,
legend.text.size = 0.7
)
# inspect
sg_map
# create side-by-side plot
# tmap_arrange(rr_map, sg_map, ncol = 2, nrow = 1)
# exceedance probabilities allows the user the quantify the levels of uncertainty surrounding the riks we quantified
# we can use a threshold for instance RR > 1 and ask what is the probability that an area has an excess risk of suicide casualties and viualise as well
# for this extraction, we need to use functions from the tidybayes and tidyverse packages i.e. spread_draws(), group_by(), summarise(), and pull()
# extract the exceedance probabilities from the icar_poisson_fit object and compute the probability that an area has a relative riks raio > 1.0
threshold <- function(x){mean(x > 1.00)}
exceedance_probability <- icar_poisson_fit %>%
spread_draws(rr_mu[i]) %>%
group_by(i) %>%
summarise(rr_mu=threshold(rr_mu)) %>%
pull(rr_mu)
# insert the exceedance values into model_df
model_df$exceedance_probability <- exceedance_probability
# create the labels for the probabilities
prob_cat_list <- c("<0.01", "0.01-0.09", "0.10-0.19", "0.20-0.29", "0.30-0.39", "0.40-0.49","0.50-0.59", "0.60-0.69", "0.70-0.79", "0.80-0.89", "0.90-0.99", "1.00")
# categorising the probabilities in bands of 10s
model_df$prob_cat <- NA
model_df$prob_cat[model_df$exceedance_probability>=0 & model_df$exceedance_probability< 0.01] <- 1
model_df$prob_cat[model_df$exceedance_probability>=0.01 & model_df$exceedance_probability< 0.10] <- 2
model_df$prob_cat[model_df$exceedance_probability>=0.10 & model_df$exceedance_probability< 0.20] <- 3
model_df$prob_cat[model_df$exceedance_probability>=0.20 & model_df$exceedance_probability< 0.30] <- 4
model_df$prob_cat[model_df$exceedance_probability>=0.30 & model_df$exceedance_probability< 0.40] <- 5
model_df$prob_cat[model_df$exceedance_probability>=0.40 & model_df$exceedance_probability< 0.50] <- 6
model_df$prob_cat[model_df$exceedance_probability>=0.50 & model_df$exceedance_probability< 0.60] <- 7
model_df$prob_cat[model_df$exceedance_probability>=0.60 & model_df$exceedance_probability< 0.70] <- 8
model_df$prob_cat[model_df$exceedance_probability>=0.70 & model_df$exceedance_probability< 0.80] <- 9
model_df$prob_cat[model_df$exceedance_probability>=0.80 & model_df$exceedance_probability< 0.90] <- 10
model_df$prob_cat[model_df$exceedance_probability>=0.90 & model_df$exceedance_probability< 1.00] <- 11
model_df$prob_cat[model_df$exceedance_probability == 1.00] <- 12
# check to see if legend scheme is balanced
table(model_df$prob_cat)
View(model_df)
# create the labels for the probabilities
prob_cat_list <- c("<0.01", "0.01-0.09", "0.10-0.19", "0.20-0.29", "0.30-0.39", "0.40-0.49","0.50-0.59", "0.60-0.69", "0.70-0.79", "0.80-0.89", "0.90-0.99", "1.00")
# categorising the probabilities in bands of 10s
model_df$prob_cat <- NA
model_df$prob_cat[model_df$exceedance_probability>=0 & model_df$exceedance_probability< 0.01] <- 1
model_df$prob_cat[model_df$exceedance_probability>=0.01 & model_df$exceedance_probability< 0.10] <- 2
model_df$prob_cat[model_df$exceedance_probability>=0.10 & model_df$exceedance_probability< 0.20] <- 3
model_df$prob_cat[model_df$exceedance_probability>=0.20 & model_df$exceedance_probability< 0.30] <- 4
model_df$prob_cat[model_df$exceedance_probability>=0.30 & model_df$exceedance_probability< 0.40] <- 5
model_df$prob_cat[model_df$exceedance_probability>=0.40 & model_df$exceedance_probability< 0.50] <- 6
model_df$prob_cat[model_df$exceedance_probability>=0.50 & model_df$exceedance_probability< 0.60] <- 7
model_df$prob_cat[model_df$exceedance_probability>=0.60 & model_df$exceedance_probability< 0.70] <- 8
model_df$prob_cat[model_df$exceedance_probability>=0.70 & model_df$exceedance_probability< 0.80] <- 9
model_df$prob_cat[model_df$exceedance_probability>=0.80 & model_df$exceedance_probability< 0.90] <- 10
model_df$prob_cat[model_df$exceedance_probability>=0.90 & model_df$exceedance_probability< 1.00] <- 11
model_df$prob_cat[model_df$exceedance_probability == 1.00] <- 12
# check to see if legend scheme is balanced
table(model_df$prob_cat)
View(model_df)
# map of exceedance probabilities
tm_shape(model_df) +
tm_fill("prob_cat", style = "cat", title = "Probability", palette = "GnBu", labels = prob_cat_list) +
tm_shape(model_df) +
tm_polygons(alpha = 0.05, border.col = "black") +
# tm_text("name", size = "AREA") +
tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7) +
tm_compass(position = c("right", "top")) +
tm_scale_bar(position = c("right", "bottom"))
# the map display exceedance probabilities for suicide risk across South Korea, providing a spatial visualisation of regions where the risk surpasses a given threshold
# the probability values range from <0.01 (very low risk) to 1.00 (very high risk), illustrating the heterogeneity of suicide risk across different administrative areas
# Seoul and surrounding metropolitan areas exhibit lower exceedance probabilities (0.01 - 0.39)
# this suggests that the relative suicide risk is lower in the capital regions compared to other parts of the country
# the majority of the country, particularly rural and peripheral regions, exhibit higher exceedance probabilities (0.50 - 1.00)
# metropolitan cities in general show lower levels of suicide risk compared to those of rural areas
# there seems to be a strong evidence of spatial clustering
# begin by loading necessary libraries
library(here)
library(tidyverse)
library(descr)
library(ggplot2)
library(scales)
library(stargazer)
# read in the Understanding Society (UKHLS) dataset
ukhls <- read_csv(here::here("data", "UKHLS_prac.csv"))
# begin by loading necessary libraries
library(here)
library(tidyverse)
library(descr)
library(ggplot2)
library(scales)
library(stargazer)
# begin by loading necessary libraries
library(here)
here
here()
